\chapter{Testing}

An important part of development is testing. It is hard to test a network
protocol, because a lot of code depends on the network state and temporary
conditions. Also, especially for testing routing protocols, you need multiple
networks, and run more than one instance of the routing daemon.

There are several tools that can virtualize the network topology. Most of them are
based on virtual machines or containers. This is useful for configuration or
performance testing, but not that much for development, so we have created
a~lightweight testing framework based on Linux network namespaces to suit our
needs.

While developing the multicast routing features, we continuously tested it in
various testing environments, targeting the concrete feature being developed.
The IGMP implementation is tested against Linux kernel, which works as the IGMP
host.

It is more complicated with the PIM protocol, because we haven't
found any software solution doing the bidirectional variant of PIM. There are
routers from Cisco or Juniper which are able to manage a group in bidirectional
mode, but we haven't been provided any to test our implementation against it.
The network analyzer Wireshark is able to parse even bidirectional PIM packets,
and we've examined the packets our implementation sends whether they are
constructed correctly. Of course, the PIM implementation in BIRDs works
correctly against itself.

\section{Netlab}
Our testing framework is called the \emph{netlab}. It is published in its
separate repository on Github.\footnote{https://github.com/Aearsis/netlab}
It is inspired by the testing workflow of the BIRD team members. They were
already using network namespaces with a collection of scripts. We have decided
to clean up the scripts and give them an interface. New ideas to simplify the
development process came soon after.

To use this framework, you must clone the repository into the root of the BIRD
repository, or change the path to BIRD inside the main executable of netlab.
Then, testing the BIRD, even in complex setups, is as simple as:

\begin{lstlisting}
$ sudo netlab/netlab
[netlab] # net_up
[netlab] # start
[netlab] # log r1
[netlab] # restart r1
[netlab] # stop
[netlab] # net_down
\end{lstlisting}

\noindent You can describe the testing network using a simple declarative language. For
example, a setup running two BIRDs connected directly by an Ethernet cable:

\begin{lstlisting}
NETLAB_CFG="$NETLAB_BASE/cfg"

netlab_node r1
netlab_node r2
if_veth r1 r2 10.10.1
\end{lstlisting}

\noindent It is very simple to add bridges, dummy interfaces representing subnets, or to
create a~namespace where to run other programs sending and receiving multicast
packets. This is the network we've done the most of the testing on:

\begin{figure}[htp]
\centering
\includegraphics[scale=1.00]{img/netlab.pdf}
\caption{The network scheme we tested on}
\label{netlab}
\end{figure}

Moreover, every node has a dummy subnet, which is not drawn in the scheme.
This is a very simple network, but you can test a lot of things there. The DF
election is interesting at the \ttt{br1}, because \ttt{r2n1} and \ttt{r2n2}
have the same upstream metric. If you send packets from the \ttt{send} node, you should see
a traffic on the \ttt{br0}, but not on the dummy subnet at \ttt{r1n2}.

Currently, the netlab uses deterministic names. When the node name is
\ttt{node}, then a namespace \ttt{netlab-node} will be created. When you define
an Ethernet pair from \ttt{r1} to \ttt{r2}, then it will create an interface
\ttt{veth-r2} in the \ttt{netlab-r1} namespace, and \ttt{veth-r1} in the
\ttt{netlab-r2} namespace. This naming convention is very convenient, you can
see immediately to which node is the interface connected.

\section{Testing multicast}
To actually test the multicast forwarding, we have used simple Python scripts.

\begin{lstlisting}[language=python]
#!/usr/bin/env python
import socket,struct,time

def setup(ga):
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 32)
    mreq = struct.pack("4sl", socket.inet_aton(ga), socket.INADDR_ANY)
    sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)
    return sock

def send(ga, port, msg):
    sock = setup(ga)
    bmsg = msg.encode('us-ascii')
    while 1:
        sock.sendto(bmsg, (ga, port))
        time.sleep(0.1)

def recv(ga, port):
    sock = setup(ga)
    sock.bind((ga, port))
    while 1:
            data, addr = sock.recvfrom(1024)
            print(data.decode('us-ascii'))

# send.py
send('224.42.42.42', 42000, "What hath god wrought!");

# recv.py
recv('224.42.42.42', 42000)
\end{lstlisting}

\noindent For example, running the send script in the \ttt{send} namespace in the setup
above, the script joins a group and then starts multicasting a lot of messages.
If you run a~packet sniffer, for example \ttt{tcpdump}, you should see the traffic
traveling upstream, stopping at the RPL. When you run the receiving script in
the \ttt{recv} namespace, messages should start coming. You can use the netlab
for this purpose:

\begin{lstlisting}
[netlab] # shell send
[netlab send] # ./send.py &
[netlab send] # disown && exit
[netlab] # shell r1
[netlab r1] # tcpdump -i veth-br0
\end{lstlisting}

\noindent When testing, please be aware that PIM needs a path to the RP to work. In the
testing environment an OSPF instance is taking care of this, and it can take
a~minute to initialize.
