\chapter{Multicast in the BIRD}

When we designed the multicast routing features into BIRD, we tried to respect
local customs. We have separated the synchronization of routing tables with the
kernel, which is system-dependent, into a protocol named \emph{mkernel}.
Another standalone part is the IGMP protocol, which is independent on the
particular multicast routing protocol used. The PIM protocol itself could be
further divided into parts, but it would require the network administrator to
understand the internal structure, making it significantly harder to configure.

We decided, for now, to support IPv4 only. Supporting IPv6 would require adding
another two protocols, because the kernel interface is separated and the IGMP is
IPv4-specific. For IPv6 there is a protocol called Multicast Listener Discovery
(MLD), and though they share common principles with IGMP, they differ not only
in the address length. The PIM protocol is designed with IPv6 in mind.
\nomenclature{MLD}{Multicast Listener Discovery}

\section{The IGMP}

Our implementation of IGMP is straightforward. It resides in
\ttt{/proto/igmp/} and it is split into three files: \ttt{igmp.h},
\ttt{igmp.c} and \ttt{packets.c}.

In the last, one, we can find a packet processing code. As there are only two
packet types in IGMPv2, this code looks simpler than similar code in other
protocols. Because there can be only one IGMP control socket which receives all
the packets, we have decided to open the control socket in the mkernel protocol
and emulate packet receiving for IGMP instances.

Because IGMP needs to send packets themselves, we decided to dedicate a~special
type of BIRD sockets for them -- \ttt{SK\_IGMP}. When a protocols opens
a socket of this type, an actual raw IP socket with protocol
\ttt{IPPROTO\_IGMP} is opened under the hood. But this socket is used for
sending packets only, its read buffer is not allocated and the socket itself is
not added to the main IO loop. Instead, depending on whether it is specific to
one interface or not, it is added (``registered'') to one of the internal lists
in the mkernel protocol. Mkernel has its IGMP control socket opened, and it calls
\ttt{rx\_hook} for every registered socket. It temporarily substitutes these
protocols' read buffer with the shared one to avoid copying the packet data, so
read hooks should not modify the packet.

As in IGMPv2, every packet consists of a common header only, a simple
\ttt{struct igmp\_pkt} is fitted onto the received data and read. Similarly,
when sending a~packet, this structure is filled and sent as a sequence of bytes.

The IGMP instance keeps a state structure \ttt{struct igmp\_iface} for every
interface known. It contains a socket for sending multicast packets there and holds
the state machine with all the timers. This state machine ensures there is only
one querying router on every interface. Next, there is a hash table of joined
groups.

There are four states for every group according to the standard. One of them,
``no members'' is implicit by not having any state information for this group.
An~existing group state for a group with no members may exist only temporarily.
In the other states, \ttt{struct igmp\_grp} exists for such group with all
the timers needed. Internal state changes do not affect routing.

The IGMP produces ``multicast requests'' -- tuples of $(S, G, I)$ with the
meaning that there is a host which joined $(S, G)$ on interface $I$. These
requests are stored in a table common for multiple protocols. This way
protocols communicate, and it allows us to provide a PIM implementation
independent on the IGMP, and vice versa.

These tables are just a special kind of routing tables. This matches the
paradigm of the BIRD made of routing tables and protocols, and it comes with
surprising benefits. For this purpose, internal routing tables were generalized
to use \ttt{struct net\_addr} as a key. This structure can be inherited and
its methods adapted -- as in case of \ttt{net\_addr\_mreq}. While the most
used \ttt{net\_addr} stores a~network prefix (hence its name),
\ttt{net\_addr\_mreq} stores group address and interface. When a ``route''
exists for $(G, I)$, then the routing protocol should deliver the traffic for
$G$ to the interface $I$. These multicast requests do not have their kernel
counterpart.

One of the benefits of having this information in routing table is that you can
reuse BIRD's powerful mechanisms to mangle routes. You can for example set
these request statically by adding a static route. Or you can exclude one
specific group using route filters. You can have more request tables and run
independent instances of the IGMP connected to different multicast routing
protocols. Or, when it does make sense, you can even use the BIRD to manage one
group with different routing protocols on different interfaces.

\section{The PIM}

As already said, we have implemented only one multicast routing protocol to the
BIRD, that is PIM in its bidirectional variant. This variant is simpler than
the others, but still needs to hold a lot of state in a way different from
unicast routing protocols. Because of that, a great portion of PIM code only
manages its internal state. Despite implementing only this variant, we have
thought of other PIM variants and the internal structure is as
future-proof as possible.

You can find the PIM implementation in \ttt{/proto/pim/}. It is split into four
parts: \ttt{pim.c}, \ttt{pim.h}, \ttt{packets.c} and \ttt{df.c}. Let us first
look at the header file. There are lots of structure definitions for holding
the internal state.

\begin{description}[style=nextline]
\item[\ttt{pim\_proto}]
  This is the instance of the protocol.

\item[\ttt{pim\_iface}]
  For every interface PIM knows, we need to open a socket. Information needed
  to send hello packets is stored here. These are stored in a list
  \ttt{pim\_proto->ifaces}. Even the kernel keeps all interfaces in a list, so
  this should be safe and fast enough. Moreover, in every socket there is
  a pointer to this structure, so we do not need to traverse the list on every
  packet received.

\item[\ttt{pim\_rp}]
  Keeps the shared tree information. All instances of this structure are stored
  in \ttt{pim\_proto->rp\_states}, which is a hash table indexed by the RPA. The
  protocol design assumes a lot of groups share a RP, so there should be only
  a few of these. There is no way how to abandon a RP tree, so this state is
  never deleted.

  RP state caches its upstream and metric towards RP, so we do not have to
  search in the MRIB often.

\item[\ttt{pim\_rp\_iface}]
  For every shared RP tree and every interface there is a DF elected, so we
  keep an instance for every member of the cartesian product of \ttt{pim\_iface} and
  \ttt{pim\_rp}. All of them are stored in a hash table in \ttt{pim\_proto},
  because the DF election is bursty and fast, so we want to find these state
  structures quickly.

\item[\ttt{pim\_grp}, \ttt{pim\_grp\_iface}]
  The PIM join/prune mechanism maintains a group state over the RP tree. We
  need to remember which downstream branches have joined, and keep a~timer to
  join upstream. We keep the upstream state (\ttt{pim\_grp}) only when we're
  joined upstream, and the downstream state  (\ttt{pim\_grp\_iface}) only when
  there's a joined router downstream.

  The \ttt{pim\_grp} instances are kept in a hash table in \ttt{pim\_proto}, but the
  downstream states are kept in a list inside the upstream state. Iterating
  over this list must be cheap anyway, because we need to forward every single
  packet to all these interfaces.

\item[\ttt{pim\_neigh}]
  We maintain a direct-neighbor cache. For every neighbor, we need to remember
  the flags it announced in its hello message, and its generation identifier.

\end{description}

\noindent Other structures are either too trivial to explain or just temporary
to share information among PIM compile units.

The PIM itself is split into three logical units, which share a common state.
The join/prune mechanism, the DF election mechanism and the forwarding logic.
You can see how the units communicate on the figure \ref{pim-units}.

\begin{figure}[htp]
\centering
\includegraphics[scale=1.00]{img/pim.pdf}
\caption{A scheme of the PIM protocol}
\label{pim-units}
\end{figure}

\subsection{The join/prune mechanism}
When the routers distribute traffic over the RP tree, they need to know which
branches do have receivers for a group $G$. The join/prune mechanism is connected to
the multicast requests table, and uses the channel in both ways. Everything
starts, when a hosts joins a group by sending an IGMP membership report. The
first-hop router notes it, and adds an entry to the request table. From there it is
propagated to the join/prune mechanism of an active PIM instance.

As it is the first join for a group $G$, the \ttt{pim\_grp} structure for $G$ is
created. Existence of the upstream state results in sending a Join to the
\emph{upstream neighbor} -- the Designated Forwarder on the upstream interface.
When the upstream neighbor receives a join on downstream, it creates the
downstream state, where it keeps an expiry timer. Then it adds an entry to the
multicast request table. The entry is then, being a route, announced back to
the PIM. Because the PIM does not make a difference between different multicast
requests, it is essentially the same situation as on the downstream router.

\label{why-pim-mreq}
This way the mechanism is split into two layers -- downstream, receiving PIM
joins, and upstream, propagating any joins to the upstream neighbor. Of course,
looping the simple request through the BIRD's routing tables introduces a small
delay, but you can alter its behavior using filters.

The main domain of the BIRD is its ability to run multiple routing protocols at
the same time. While routing an unicast packet through multiple autonomous
systems with different routing protocols is possible, well defined and widely
used, it is not so easy with multicast. One way how to make it work is through
this request sharing.

An interesting implementation detail is that BIRD can announce routes in two
complementary ways. Either you can let it announce any route (\ttt{RA\_ANY}),
then it will call the \ttt{rt\_notify} hook with the old and new route version,
or you can ask for optimal route updates only (\ttt{RA\_OPTIMAL}), which will
announce the old and new optimal route for a network prefix. Having the optimal
mode on the request table will result in only three possible combinations of
the old and new route. When the old one is \NULL, and a new one exists, this is a new
request and we need to create the downstream state. When the old one exists and
the new one is \NULL, the last request for this group and interface vanished,
and it is safe to stop forwarding. When both the old and new route exists, we
do not have to do anything, because we already consider this interface joined.

\subsection{The DF election mechanism}
On every interface and for every RPA known, an election takes place. The
election is based on the routers shouting their metric, and whoever shouts the
most is the winner. The mechanism is rather complicated to avoid routing loops.
Because of it, a state machine with a lot of transition edges is defined. The
behavior of this machine is separated from the main code into the file
\ttt{/proto/pim/df.c}.

The majority of transition edges are based on an incoming packet. Every packet
has a metric attached, there are four types of packets and four states of the
state machine. To handle those 32 cases, there is a huge \ttt{switch}
statement, which uses macros to combine all three variables into one number,
and a few other macros to keep the switch as small as possible while retaining
readability.

Whenever a packet comes, there is a quick lookup in the hash table to find the
state structure \ttt{pim\_rp\_iface}. Then the \ttt{pim\_df\_message} method is
called. Responses are sent immediately.

This part required a notable change in BIRD's IO loop. Previously, the BIRD's
timers had a precision of one second only. The reasons for this were explained
in the section \ref{bird-internals}. The DF election must be fast, its
messages are spaced by $100\,$ms by default, so the BIRD timers were not
applicable. There already is a protocol which is more time sensitive, the BFD. It
solves this problem by having a separate IO loop. But BFD does not alter the
routing tables, and BIRD internals are not thread-safe, so having a separate
loop for PIM is not an option.

Because complete rewrite of the main loop is planned, we have decided only to
temporarily hack the existing timers. It had to be done carefully not to break
existing protocols. The solution was to add another field to the timer
structure, keeping a \ttt{btime}. That is an already used BIRD time abstraction, with
defined constants for most used time fractions. Because existing protocols
sometimes use the original field, we have kept it there. Then, we've added new
methods to work with these timers with a microsecond precision:
\ttt{tm\_start\_btime}, \ttt{tm\_remains\_btime} and
\ttt{tm\_start\_min\textrm{/}max\_btime}. The old respective ones were changed
to convert seconds to \ttt{btime} and call the new methods. This way all the timers
have the \ttt{btime} precision, but the old API was kept the same, not breaking
anything.

\subsection{The forwarding logic}
Having the RP tree built and knowing which branches we want to forward traffic
to, the forwarding logic is pretty straightforward. But first, let us explain
how we represent multicast routes. As in case of the multicast request,
multicast routes are another special kind of ``networks'', and they have their
routing tables. The network type is called \ttt{mgrp4} or \ttt{mgrp6},
depending on the IP version.

Multicast routes are slightly different from the unicast ones. In unicast routing,
the most common route has a next hop router, only the last-hop routers have
a~direct device route. As multicasts are always sent directly to link, no such
thing as a next-hop route exists. Furthermore, you often want to send a packet in
multiple copies to several interfaces. We have discussed a bit how to control
multicast routing in Linux in section \ref{mcast-kernel}, but we want to keep
the internal tables system-independent.

Every interface, which you want to use for multicast routing, must be assigned
an index. Those reflect the VIF indices, but the implementation does not depend
on them and can be changed any time. In order to assign an index, you must enable the
interface for the mkernel protocol. If you don't, packets won't be
forwarded, but you will not be warned. This is the expected behavior, because
you probably do not want to forward traffic there, when you have explicitly
disabled the interface for the mkernel.

From the theoretical point of view, the most generic representation for
a~multicast routing table could be a mapping of $(S, G, I) \rightarrow O$,
having a set of outgoing interfaces $O$ for every source $S$, group $G$ and
input interface $I$. This is however too generic and space-consuming, so we
have made it simpler. First, there is no need to store a source yet, because we
do not support source-specific multicast routing. Without the restriction to
one source, there are often more incoming interfaces, for which the outgoing
set is almost identical (typically having a common superset, from which the
incoming interface is removed). Such routes can be merged, keeping a set of
incoming interfaces. The outgoing set is often a subset of the incoming set.
There could possibly be two disjoint sets of input interfaces, for which the
output interfaces are different, but that is something we do not want to allow.

The final representation of multicast routing table we decided to implement is
a mapping of $G \rightarrow (\mathrm{iifs}, \mathrm{oifs})$, having a set of feasible incoming
interfaces and set of outgoing ones for every group. When a packet comes on
interface $I \in \mathrm{iifs}$ for a group $G$, forward it to every $O \in \mathrm{oifs}, O \neq I$.
These sets are represented as bitmaps in the route. The bits are indexed by the
interface index assigned by mkernel.

Having a multicast routing table, the forwarding logic of PIM is only a simple
filtering aggregator of multicast requests. Whenever a routing change occurs,
it constructs a new route for $G$, having all interfaces where the router is
acting DF in incoming interfaces, and every interface for which a multicast
requests exists and the router is an acting DF in outgoing interfaces. Such
a route is then announced to the routing table. When the group state is
removed, a \NULL{} route is announced.

\section{The mkernel protocol}
The protocol itself is hidden in the system-dependent source subtree, in the
file \ttt{/sysdep/unix/mkrt.c} and corresponding header file. We have tried to
keep all inconveniences in the
kernel design there. Because we need to reflect how kernel thinks of multicast
routing, we were restricted in designing this protocol.

First of all, you cannot run more than one instance of this protocol. This is
subject to change, because it is possible to have more than one kernel
multicast routing table on some operating systems. Even when multiple tables
are involved, there must be exactly one control socket opened. One task of the
mkernel is to internally redistribute IGMP packets received there.

Another thing mkernel does is assigning the interface multicast indices. It
uses a simple algorithm, keeping a bitmap of used indices, and assigning the
first one available. Using this identifier, a VIF with the same index is created.
Obviously, to assign an index, you have to pass this interface to the protocol.
Because zero is a valid index too, when the index is assigned to a \ttt{struct
iface}, a flag \ttt{IF\_VIFI\_ASSIGNED} is added, too.

The main purpose of this protocol is the synchronization of the routing table. If you
look at the overview of kernel routes in the first chapter, you won't find
a~type similar to those we're using. To make matters worse, none of them is
usable for our purposes. $(S, G)$ routes obviously work only when we know the
source, and $(*,G)$ routes cannot limit the incoming interfaces per group.
Ignoring this limitation would cause routing loops even in common scenarios.

When the kernel does not find a matching MFC entry, it calls the userspace daemon
to resolve the situation. In the PIM-SM mode, this often results in
encapsulating the packet, but not in PIM-BIDIR. The so-called \emph{upcall}
consists of sending an artificial IGMP packet with a packet type unused in the
IGMP standard. The constants are defined in \ttt{<linux/mroute.h>}. Three
upcalls can be found in the linux kernel source code:

\bgroup
\renewcommand{\arraystretch}{1.5}
\bigskip
\noindent\begin{tabularx}{\textwidth}{lX}
  \ttt{IGMPMSG\_NOCACHE} & There is an MFC miss. This is the only upcall we need
  to listen to. \\
  \ttt{IGMPMSG\_WRONGVIF} & An MFC entry exists, but different incoming
  interface was expected. This upcall is sent only in one specific situation
  when switching the RP and the source-specific tree in PIM-SM. In other
  situations, this upcall is suppressed. \\
  \ttt{IGMPMSG\_WHOLEPKT} & An MFC miss occurred, but the kernel is configured to pass us
  the whole incoming packet for encapsulation. \\
\end{tabularx}
\bigskip
\egroup

When an MFC miss occurs, the packet is enqueued in the kernel. If the MFC entry
is added fast enough, it will be resolved and forwarded afterwards. Because
these packets are ordinary IGMP packets, but it does not make sense to process
them in the IGMP protocol, we are forced to partially parse the IGMP packet in
the mkernel protocol. If it is one of these upcalls, resolve it now, otherwise
pass it to the IGMP protocol(s).

While resolving an MFC entry, we look up the route for the group, check if the
incoming interface is in the iifs, and add a route specifically for this
source. Because $(S,G)$ routes use a RPF check, we can control both iifs and
oifs. The main problem in this method is reflecting changes in routing. When
a route changes in BIRD, we are announced the group, but we need to change all
the MFC entries for specific sources we have added. That means we have to
remember what sources we've added and delete everything on every route change.

Also, without exposing the group to RPA mapping, we cannot route groups without
an upstream state in PIM. This results in an inconvenient but reasonable
restriction -- every source must join the group it sends packets to.
